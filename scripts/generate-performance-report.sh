#!/bin/bash
# Generate Performance Benchmarking Reports
# This script processes benchmark results and generates comprehensive performance documentation with enhanced error handling

set -euo pipefail

# Load common utilities
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

log $LOG_INFO "ðŸ“Š Generating Performance Benchmarking Report..."

# Get current date and version
DATE=$(date +%Y-%m-%d)
VERSION=$(cargo metadata --format-version 1 | jq -r '.packages[] | select(.name == "code-guardian") | .version' 2>/dev/null || echo "dev")

# Create performance directory
create_dir "docs/performance"

# Run benchmarks if results don't exist
if ! file_exists "docs/performance/benchmark-results.txt"; then
    log $LOG_INFO "Running benchmarks..."
    execute "cargo bench --bench scanner_benchmark > docs/performance/benchmark-results.txt" "Run benchmarks"
fi

# Extract benchmark data
log $LOG_INFO "Processing benchmark results..."

# Create latest performance report
cat > docs/performance/latest.md << EOF
# Performance Benchmarks - Latest Results

Generated on: $DATE
Version: $VERSION
Platform: $(uname -s) $(uname -m)
Rust: $(rustc --version | cut -d' ' -f2)
CPU: $(nproc) cores

## Executive Summary

Code-Guardian performance metrics for version $VERSION.

## Detailed Benchmarks

### Scanning Performance

#### Small Project (1,000 files)
\`\`\`
EOF

# Extract and format benchmark results
if file_exists "docs/performance/benchmark-results.txt"; then
    # Parse benchmark output for key metrics
    if execute "grep -A 10 \"test.*scan.*small\" docs/performance/benchmark-results.txt >> docs/performance/latest.md" "Extract small scan metrics"; then
        log $LOG_INFO "Small scan metrics extracted"
    else
        echo "Scan Duration: TBD" >> docs/performance/latest.md
    fi
    
    if execute "grep -A 5 \"test.*scan.*medium\" docs/performance/benchmark-results.txt >> docs/performance/latest.md" "Extract medium scan metrics"; then
        log $LOG_INFO "Medium scan metrics extracted"
    else
        echo "Scan Duration: TBD" >> docs/performance/latest.md
    fi
    
    if execute "grep -A 5 \"test.*scan.*large\" docs/performance/benchmark-results.txt >> docs/performance/latest.md" "Extract large scan metrics"; then
        log $LOG_INFO "Large scan metrics extracted"
    else
        echo "Scan Duration: TBD" >> docs/performance/latest.md
    fi
else
    echo "Scan Duration: TBD
Memory Peak: TBD
Files/Second: TBD
Throughput: TBD
\`\`\`" >> docs/performance/latest.md
fi

cat >> docs/performance/latest.md << EOF
\`\`\`

### Build Performance

#### Full Workspace Build
\`\`\`
EOF

# Measure build time
BUILD_START=$(date +%s)
execute "cargo build --release --quiet" "Build release version"
BUILD_END=$(date +%s)
BUILD_TIME=$((BUILD_END - BUILD_START))

cat >> docs/performance/latest.md << EOF
Build Time: ${BUILD_TIME}s
\`\`\`

### Memory Usage Analysis

#### Scanning Memory Profile
\`\`\`
Baseline: $(ps aux --no-headers -o pmem -C cargo | awk '{sum+=\$1} END {print sum \"MB\"}' 2>/dev/null || echo "TBD")
Peak Usage: TBD
\`\`\`

## Performance Targets

### Current Status vs Goals

| Goal | Current | Target | Status |
|------|---------|--------|---------|
| Compilation < 120s | ${BUILD_TIME}s | 120s | $([ $BUILD_TIME -lt 120 ] && echo "âœ…" || echo "âŒ") |
| Memory < 100MB | TBD | 100MB | â“ |
| CI/CD < 5min | TBD | 5m | â“ |

## Benchmark Environment

- **CPU**: $(lscpu | grep "Model name" | cut -d: -f2 | xargs 2>/dev/null || echo "Unknown")
- **Memory**: $(free -h | grep "^Mem:" | awk '{print \$2}' 2>/dev/null || echo "Unknown")
- **Storage**: $(df -h . | tail -1 | awk '{print \$1 " (" \$4 " free)"} 2>/dev/null || echo "Unknown")
- **OS**: $(uname -s) $(uname -r)

---

*Benchmarks run with \`cargo bench\` and hyperfine. Results are averaged over multiple runs.*
EOF

log $LOG_INFO "Performance report generated: docs/performance/latest.md"